package com.sparrowrecsys.offline.spark.embedding

import java.io.{BufferedWriter, File, FileWriter}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.ml.feature.BucketedRandomProjectionLSH
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.expressions.UserDefinedFunction
import org.apache.spark.sql.functions._
import org.apache.spark.sql.{Row, SparkSession}
import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer
import scala.util.Random
import scala.util.control.Breaks.{break, breakable}

object EmbeddingLargeDataset {

  // å¤„ç†ç‰©å“åºåˆ—æ•°æ® - ä½¿ç”¨æ–°çš„å¤§æ•°æ®é›†
  def processItemSequence(sparkSession: SparkSession, ratingsPath: String): RDD[Seq[String]] = {
    println(s"æ­£åœ¨åŠ è½½æ•°æ®é›†: $ratingsPath")
    
    val ratingSamples = sparkSession.read
      .format("csv")
      .option("header", "true")
      .load(ratingsPath)

    println("æ•°æ®é›†Schema:")
    ratingSamples.printSchema()
    
    println(s"æ€»è¯„åˆ†æ•°é‡: ${ratingSamples.count()}")

    // æŒ‰æ—¶é—´æˆ³æ’åºçš„UDF
    val sortUdf: UserDefinedFunction = udf((rows: Seq[Row]) => {
      rows.map { case Row(movieId: String, timestamp: String) => (movieId, timestamp) }
        .sortBy { case (_, timestamp) => timestamp.toLong }
        .map { case (movieId, _) => movieId }
    })

    // ç­›é€‰é«˜è¯„åˆ†(>=4.0)å¹¶ç”Ÿæˆç”¨æˆ·è¡Œä¸ºåºåˆ—
    val userSeq = ratingSamples
      .where(col("rating") >= 4.0)  // æé«˜è¯„åˆ†é˜ˆå€¼
      .groupBy("userId")
      .agg(sortUdf(collect_list(struct("movieId", "timestamp"))) as "movieIds")
      .withColumn("movieIdStr", array_join(col("movieIds"), " "))
      .filter(size(col("movieIds")) >= 5)  // è‡³å°‘5ä¸ªè¯„åˆ†è®°å½•

    println("ç”Ÿæˆçš„ç”¨æˆ·åºåˆ—æ ·ä¾‹:")
    userSeq.select("userId", "movieIdStr").show(5, truncate = false)
    
    println(s"æœ‰æ•ˆç”¨æˆ·æ•°é‡: ${userSeq.count()}")

    userSeq.select("movieIdStr").rdd.map(r => r.getAs[String]("movieIdStr").split(" ").toSeq)
  }

  // è®­ç»ƒItem2Vecæ¨¡å‹ - ä¼˜åŒ–å‚æ•°
  def trainItem2vecLarge(sparkSession: SparkSession, samples: RDD[Seq[String]], embLength: Int, embOutputFilename: String): Word2VecModel = {
    println("å¼€å§‹è®­ç»ƒItem2Vecæ¨¡å‹...")
    
    val word2vec = new Word2Vec()
      .setVectorSize(embLength)
      .setWindowSize(10)        // å¢å¤§çª—å£å¤§å°
      .setNumIterations(20)     // å¢åŠ è¿­ä»£æ¬¡æ•°
      .setMinCount(5)           // æœ€å°‘å‡ºç°æ¬¡æ•°

    val model = word2vec.fit(samples)
    
    println(s"è®­ç»ƒå®Œæˆ! è¯æ±‡è¡¨å¤§å°: ${model.getVectors.size}")

    // æµ‹è¯•ç›¸ä¼¼æ€§
    if (model.getVectors.contains("1")) {
      val synonyms = model.findSynonyms("1", 10)
      println("ç”µå½±ID=1çš„ç›¸ä¼¼ç”µå½±:")
      for ((synonym, cosineSimilarity) <- synonyms) {
        println(f"  ç”µå½±ID: $synonym, ç›¸ä¼¼åº¦: $cosineSimilarity%.4f")
      }
    }

    // ä¿å­˜embedding
    val embFolderPath = this.getClass.getResource("/webroot/modeldata/")
    val file = new File(embFolderPath.getPath + embOutputFilename)
    val bw = new BufferedWriter(new FileWriter(file))
    
    println(s"ä¿å­˜embeddingåˆ°: ${file.getAbsolutePath}")
    
    for (movieId <- model.getVectors.keys) {
      bw.write(movieId + ":" + model.getVectors(movieId).mkString(" ") + "\n")
    }
    bw.close()

    println(s"âœ… Embeddingå·²ä¿å­˜åˆ°: $embOutputFilename")
    
    model
  }

  // ç”Ÿæˆç”¨æˆ·åµŒå…¥ - åŸºäºå¤§æ•°æ®é›†
  def generateUserEmbLarge(sparkSession: SparkSession, ratingsPath: String, word2VecModel: Word2VecModel, embLength: Int, embOutputFilename: String): Unit = {
    println("å¼€å§‹ç”Ÿæˆç”¨æˆ·embedding...")
    
    val ratingSamples = sparkSession.read.format("csv").option("header", "true").load(ratingsPath)
    
    val userEmbeddings = new ArrayBuffer[(String, Array[Float])]()

    // æŒ‰ç”¨æˆ·åˆ†ç»„å¹¶è®¡ç®—embedding
    ratingSamples
      .filter(col("rating") >= 4.0)
      .collect()
      .groupBy(_.getAs[String]("userId"))
      .foreach { case (userId, userRatings) =>
        var userEmb = new Array[Float](embLength)
        var movieCount = 0
        
        userRatings.foreach { row =>
          val movieId = row.getAs[String]("movieId")
          val movieEmb = word2VecModel.getVectors.get(movieId)
          
          if (movieEmb.isDefined) {
            movieCount += 1
            for (i <- userEmb.indices) {
              userEmb(i) += movieEmb.get(i)
            }
          }
        }
        
        if (movieCount > 0) {
          userEmb = userEmb.map(_ / movieCount)
          userEmbeddings.append((userId, userEmb))
        }
      }

    // ä¿å­˜ç”¨æˆ·embedding
    val embFolderPath = this.getClass.getResource("/webroot/modeldata/")
    val file = new File(embFolderPath.getPath + embOutputFilename)
    val bw = new BufferedWriter(new FileWriter(file))

    for ((userId, userEmb) <- userEmbeddings) {
      bw.write(userId + ":" + userEmb.mkString(" ") + "\n")
    }
    bw.close()

    println(s"âœ… ç”¨æˆ·Embeddingå·²ä¿å­˜: $embOutputFilename, ç”¨æˆ·æ•°é‡: ${userEmbeddings.length}")
  }

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.ERROR)

    val conf = new SparkConf()
      .setMaster("local[*]")
      .setAppName("EmbeddingLargeDataset")
      .set("spark.submit.deployMode", "client")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.adaptive.enabled", "true")
      .set("spark.sql.adaptive.coalescePartitions.enabled", "true")

    val spark = SparkSession.builder.config(conf).getOrCreate()

    try {
      // ä½¿ç”¨æ–°çš„å¤§æ•°æ®é›†
      val largRatingsPath = this.getClass.getResource("/webroot/sampledata/ml-25m/ratings.csv").getPath
      val embLength = 50  // å¢åŠ embeddingç»´åº¦

      println("ğŸš€ å¼€å§‹ä½¿ç”¨å¤§æ•°æ®é›†è®­ç»ƒembeddingæ¨¡å‹...")
      
      // å¤„ç†æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹
      val samples = processItemSequence(spark, largRatingsPath)
      
      println(s"ç”Ÿæˆçš„åºåˆ—æ•°é‡: ${samples.count()}")
      
      // è®­ç»ƒItem2Vec
      val model = trainItem2vecLarge(spark, samples, embLength, "item2vecEmb_large.csv")
      
      // ç”Ÿæˆç”¨æˆ·embeddingï¼ˆå¯é€‰ - å› ä¸ºæ•°æ®é‡å¤§ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
      // generateUserEmbLarge(spark, largRatingsPath, model, embLength, "userEmb_large.csv")
      
      println("ğŸ‰ å¤§æ•°æ®é›†embeddingè®­ç»ƒå®Œæˆ!")
      
    } catch {
      case e: Exception =>
        println(s"âŒ è®­ç»ƒå¤±è´¥: ${e.getMessage}")
        e.printStackTrace()
    } finally {
      spark.stop()
    }
  }
}
