package com.sparrowrecsys.offline.spark.embedding

import java.io.{BufferedWriter, File, FileWriter}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

object SimpleEmbeddingTrainer {

  // ç®€å•çš„æ•°æ®å¤„ç†æ–¹æ³• - åªä½¿ç”¨RDD
  def processRatingsData(sparkSession: SparkSession, ratingsPath: String): RDD[Seq[String]] = {
    println(s"å¼€å§‹å¤„ç†æ•°æ®: $ratingsPath")
    
    // è¯»å–CSVæ–‡ä»¶ä½œä¸ºæ–‡æœ¬
    val ratingsRDD = sparkSession.sparkContext.textFile(ratingsPath)
    
    // è·³è¿‡æ ‡é¢˜è¡Œå¹¶è§£ææ•°æ®
    val parsedRatings = ratingsRDD
      .filter(line => !line.startsWith("userId"))  // è·³è¿‡æ ‡é¢˜
      .map(line => {
        val parts = line.split(",")
        if (parts.length >= 4) {
          val userId = parts(0)
          val movieId = parts(1) 
          val rating = parts(2).toDouble
          val timestamp = parts(3).toLong
          Some((userId, movieId, rating, timestamp))
        } else {
          None
        }
      })
      .filter(_.isDefined)
      .map(_.get)

    println(s"è§£æçš„è¯„åˆ†è®°å½•æ•°: ${parsedRatings.count()}")

    // ç­›é€‰é«˜è¯„åˆ†è®°å½•(>=4.0)å¹¶æŒ‰ç”¨æˆ·åˆ†ç»„
    val userMovieSequences = parsedRatings
      .filter(_._3 >= 4.0)  // è¿‡æ»¤é«˜è¯„åˆ†
      .groupBy(_._1)        // æŒ‰ç”¨æˆ·IDåˆ†ç»„
      .map { case (userId, ratings) =>
        // æŒ‰æ—¶é—´æ’åºå¹¶æå–ç”µå½±IDåºåˆ—
        val movieSequence = ratings.toSeq
          .sortBy(_._4)  // æŒ‰æ—¶é—´æˆ³æ’åº
          .map(_._2)     // æå–ç”µå½±ID
        (userId, movieSequence)
      }
      .filter(_._2.length >= 5)  // è‡³å°‘5ä¸ªè¯„åˆ†

    println(s"æœ‰æ•ˆç”¨æˆ·æ•°: ${userMovieSequences.count()}")
    
    // æ˜¾ç¤ºä¸€äº›æ ·ä¾‹
    userMovieSequences.take(5).foreach { case (userId, movieSeq) =>
      println(s"ç”¨æˆ· $userId: ${movieSeq.take(10).mkString(" ")}...")
    }

    // è¿”å›ç”µå½±åºåˆ—ç”¨äºWord2Vecè®­ç»ƒ
    userMovieSequences.map(_._2)
  }

  // è®­ç»ƒItem2Vecæ¨¡å‹
  def trainSimpleItem2Vec(sparkSession: SparkSession, sequences: RDD[Seq[String]], embLength: Int, outputFile: String): Word2VecModel = {
    println("å¼€å§‹è®­ç»ƒItem2Vecæ¨¡å‹...")
    
    val word2vec = new Word2Vec()
      .setVectorSize(embLength)
      .setWindowSize(10)
      .setNumIterations(10)
      .setMinCount(5)

    val model = word2vec.fit(sequences)
    
    println(s"è®­ç»ƒå®Œæˆ! è¯æ±‡è¡¨å¤§å°: ${model.getVectors.size}")

    // æµ‹è¯•ç›¸ä¼¼æ€§
    val movieIds = model.getVectors.keys.toSeq
    if (movieIds.nonEmpty) {
      val testMovieId = movieIds.head
      try {
        val synonyms = model.findSynonyms(testMovieId, 5)
        println(s"ç”µå½± $testMovieId çš„ç›¸ä¼¼ç”µå½±:")
        synonyms.foreach { case (movieId, similarity) =>
          println(f"  $movieId: $similarity%.4f")
        }
      } catch {
        case e: Exception => println(s"ç›¸ä¼¼æ€§æµ‹è¯•å¤±è´¥: ${e.getMessage}")
      }
    }

    // ä¿å­˜æ¨¡å‹
    saveEmbedding(model, outputFile)
    model
  }

  // ä¿å­˜embeddingåˆ°æ–‡ä»¶
  def saveEmbedding(model: Word2VecModel, filename: String): Unit = {
    try {
      val embFolderPath = this.getClass.getResource("/webroot/modeldata/")
      val file = new File(embFolderPath.getPath + filename)
      val bw = new BufferedWriter(new FileWriter(file))

      println(s"ä¿å­˜embeddingåˆ°: ${file.getAbsolutePath}")
      
      for (movieId <- model.getVectors.keys) {
        bw.write(movieId + ":" + model.getVectors(movieId).mkString(" ") + "\n")
      }
      bw.close()
      
      println(s"âœ… æˆåŠŸä¿å­˜ ${model.getVectors.size} ä¸ªç”µå½±embeddingåˆ° $filename")
    } catch {
      case e: Exception => 
        println(s"âŒ ä¿å­˜embeddingå¤±è´¥: ${e.getMessage}")
    }
  }

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.ERROR)

    val conf = new SparkConf()
      .setMaster("local[*]")
      .setAppName("SimpleEmbeddingTrainer")
      .set("spark.submit.deployMode", "client")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")

    val spark = SparkSession.builder.config(conf).getOrCreate()

    try {
      println("ğŸš€ å¼€å§‹ç®€å•embeddingè®­ç»ƒ...")

      // ä½¿ç”¨å¤§æ•°æ®é›†
      val largRatingsPath = this.getClass.getResource("/webroot/sampledata/ml-25m/ratings.csv").getPath
      val embLength = 50

      // å¤„ç†æ•°æ®
      val sequences = processRatingsData(spark, largRatingsPath)
      
      println(s"ç”Ÿæˆçš„åºåˆ—æ•°: ${sequences.count()}")

      // è®­ç»ƒæ¨¡å‹
      val model = trainSimpleItem2Vec(spark, sequences, embLength, "item2vec_large_simple.csv")

      println("ğŸ‰ è®­ç»ƒå®Œæˆ!")
      
    } catch {
      case e: Exception =>
        println(s"âŒ è®­ç»ƒå¤±è´¥: ${e.getMessage}")
        e.printStackTrace()
    } finally {
      spark.stop()
    }
  }
}
